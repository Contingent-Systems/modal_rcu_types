\section{Introduction}
%Orchestrating the threads to utilize the machine resource as much as possible is main concern of a concurrent system programmer. Using lock-base mutual exclusion is expensive to follow in terms of machine resources thus concurrent programmers prefer using non-blocking stacks, linked-lists, queues which follow low-level synchronisation primitives such as compare-and-swap operations instead of lock based mutual exclusion to get a better when high contention among thread occur.
%Reasoning about concurrent programs is hard. We need to consider all interactions between threads because side-effects in one thread can affect the behaviour of another thread. When we consider non-blocking algorithms, reasoning becomes harder because interactions between threads running non-blocking algorithms are more subtle than the ones exist in lock-based mutual exclusion. There are logics and automated reasoning tools to address challenge in reasoning consistency of non-blocking algorithms[Ref to some logics for non-blocking alg].
%Normally, necessity of doing a set of memory operations simultaneously or indivisibly for correctness reasons may get polluted due to side-affects and partially completed set of operations may result in inconsistency for the clients of the data structure.
%However, there are many cases where strict consistency may be relaxed with temporal inconsistency if clients may tolerate stale data. This is mainly done with delaying any work that is not urgent to be performed. This relaxation brings new kind of programming pattern where there is no need for reader locks. Mutator processes' accesses to shared data are mutually exclusive via writer lock that prevents simultaneous update of a node by more than one process. In this concurrent setting, there is a set of reader processes and a set of updater processes with a process or set of processes to reclaim memory locations where nodes in the free list reside. An example of delaying of work occurs in garbage collection of heap locations. It is generally unnecessary to collect garbage heap locations immediately after it is generated,~\cite{Kung:1980:CMB:320613.320619}.
For many workloads, lock-based synchronization -- even fine-grained locking -- has unsatisfactory performance.  Often lock-free algorithms yield better performance, at the cost of more complex implementation and additional difficulty reasoning about the code.  Much of this complexity is due to memory management: 
developers must reason about not only other threads violating local assumptions, but whether other threads are \emph{finished accessing} nodes to deallocate. 
At the time a node is unlinked from a data structure, an unknown number of additional threads may have already been using the node, having read a pointer to it before it was unlinked in the heap.

A key insight for manageable solutions to this challenge is to recognize that just as in traditional garbage collection, the unlinked nodes need not be reclaimed immediately, but can instead be reclaimed later after some protocol finishes running.
Hazard pointers~\cite{Michael:2004:HPS:987524.987595} are the classic example: all threads actively collaborate on bookkeeping data structures to track who is using a certain reference.
For structures with read-biased workloads, Read-Copy-Update (RCU)~\cite{Mckenney:2004:EDD:1048173} provides an appealing alternative. The programming style resembles a combination of reader-writer locks and lock-free programming; readers perform minimal bookkeeping -- often nothing they wouldn't already do -- and the writer(s) perform additional work to track which readers may have observed a node they wish to deallocate.
There are now RCU implementations of many common tree data structures~\cite{urcu_ieee,Triplett:2011:RSC:2002181.2002192,DBLP:conf/asplos/ClementsKZ12,mc_report,Arbel:2014:CUR:2611462.2611471,Kung:1980:CMB:320613.320619}, and
RCU plays a key role in Linux kernel memory management~\cite{Mckenney01read-copyupdate}.
%There are now RCU variants of linked lists~\cite{urcu_ieee}, hash tables~\cite{Triplett:2011:RSC:2002181.2002192}, and balanced trees~\cite{DBLP:conf/asplos/ClementsKZ12,mc_report,Arbel:2014:CUR:2611462.2611471,Kung:1980:CMB:320613.320619,mc_report}, and RCU plays a central role in portions of the Linux kernel~\cite{Mckenney01read-copyupdate}.

However, RCU primitives remain non-trivial to use correctly: developers must ensure they release each node exactly once, from exactly one thread, \emph{after} ensuring other threads are finished with the node in question.
Sophisticated verification logics can prove correctness of the RCU primitives and clients~\cite{Gotsman:2013:VCM:2450268.2450289,fu2010reasoning,verrcu,Mandrykin:2016:TDV:3001219.3001297}.
But these techniques require significant verification expertise to apply, and are specialized to individual data structures or implementations.
One of the important reasons of the sophistication in the logics stems from the complexity of underlying memory reclamation model. However, Meyer et al.~\cite{myr} shows that a suitable approxiamtion/abstraction enables separating verifying \textit{correctness} of concurrent data structures from its underlying reclamation model under the assumption of \textit{memory safety}. We, on the other hand, are the first proving memory safety for arbitrary clients via having suitable abstractions for RCU model. Model checking can be used to validate correctness of implementations for a mock client~\cite{LiangMKM16,Desnoyers:2013:MSM:2506164.2506174,Kokologiannakis:2017:SMC:3092282.3092287,DBLP:conf/cav/AlglaveKT13}, but this does not guarantee correctness of arbitrary client code.
 %functionality in a real client, and does not scale to full clients in context.
%RCU is one programming patterns that uses delayed memory reclamation pattern,  ~\cite{Mckenney:2004:EDD:1048173}. The crucial point for safe memory management is deciding when to reclaim memory. RCU orchestrates threads reading, mutating and reclaiming a memory location in a data structure by waiting for all reader threads to finish their RCU read sections before reallocating the node. Reserachers have developed data structures following this RCU programming pattern including: linked lists ~\cite{urcu_ieee}, hash tables ~\cite{Triplett:2011:RSC:2002181.2002192} and balanced trees ~\cite{DBLP:conf/asplos/ClementsKZ12,mc_report,Arbel:2014:CUR:2611462.2611471,Kung:1980:CMB:320613.320619}. In addition, RCU is used widely in Linux kernel, and introduce in the form of the Linux \textit{Read-Copy-Update(RCU)} API ~\cite{Mckenney01read-copyupdate}. This API is used widely in modules highly concurrent read-heavy workloads to reduce synchronisation among reader threads, ~\cite{Mckenney_rcuusage}. All these make reasoning on how to implement data structures following RCU pattern, clients of RCU,  hard and any formalization and verification effort for RCU programming important, ~\cite{PaulMcKenney2005RCUSemantics, Gotsman:2013:VCM:2450268.2450289, verrcu}.

We propose a type system to ensure that RCU client code uses the RCU primitives correctly.
We do this in a general way, not assuming the client implements any specific data structure, only one satisfying some basic properties (like atree) common to RCU data structures.  In order to do this, we must also give a formal operational model of the RCU primitives that abstracts many implementations, without assuming a particular implementation of the RCU primitives.
We describe our RCU semantics and type system, prove our type system sound against the model (which ensures memory is reclaimed correctly), and show the type system in action on two important RCU data structures.
% Specifically what we have done is :
% \begin{itemize}
% \item \textbf{\textsf{Formalising RCU Semantics}}, We formalise the RCU programming model in section \textsf{3.1}.  capturing the core semantics of RCU.  We define a core language including \textsf{Read} and \textsf{Write} RCU block constructs in section \textsf{3} and operational semantics for this language in section \textsf{3.1}
% \item \textbf{\textsf{An Ownership Type System}}, We got inspired by "Multiple Aggregate Entry Points For Ownership Types" paper when deciding the fundemental theory of our type system as \textsf{Ownership-Types}. We propose a type system based on \textsf{Ownership-Types} ~\cite{Ostlund:2012:MAE:2367163.2367175} to enforce memory-safe usage of RCU primitives. To our best knowledge this is the first use of ownership types for weak models.
% \item \textbf{\textsf{Formalism, Proofs}} We formalised the type system in section \textsf{3.2}. We run type-system on Linked List and Binary Serach Tree example. We prove the soundness of the types system using \textsf{Views-Framework} ~\cite{views}.
% \end{itemize}

Our contributions include:
\begin{itemize}
\item A general (abstract) operational model for RCU-based memory management
\item A type system that ensures code uses RCU memory management correctly, which is signifiantly simpler than full-blown verification logics
\item Demonstration of the type system on two examples: a linked list and a binary search tree  
\item A proof that the type system guarantees the intended safe use of RCU primitives.
\end{itemize}
